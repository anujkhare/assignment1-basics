{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import BinaryIO\n",
    "\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO,\n",
    "    desired_num_chunks: int,\n",
    "    split_special_token: bytes,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), \"Must represent special token as a bytestring\"\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slpit on byte level, may end up with invalid utf-8 characters!\n",
    "\n",
    "Byte-level BPE (like GPT-2/GPT-4)\n",
    "In byte-level BPE, you do start with individual UTF-8 bytes as the base vocabulary. This means:\n",
    "\n",
    "The initial vocabulary consists of all 256 possible byte values (0-255)\n",
    "Multi-byte UTF-8 characters are split into their constituent bytes initially\n",
    "For example, \"café\" becomes bytes like [99, 97, 102, 195, 169] where 195,169 represents \"é\"\n",
    "\n",
    "Can this create invalid tokens? Yes, absolutely:\n",
    "\n",
    "Individual byte tokens can represent incomplete UTF-8 sequences (like having just the first byte of a multi-byte character)\n",
    "During merging, you can end up with subword tokens that contain invalid UTF-8 byte sequences\n",
    "The model learns to work with these byte-level representations, even if they don't correspond to valid Unicode characters\n",
    "\n",
    "Why byte-level BPE became popular\n",
    "Byte-level BPE is preferred in modern models because:\n",
    "\n",
    "Universal coverage: Can represent any text in any language/script\n",
    "Handles unknown characters: No need for special UNK tokens\n",
    "Consistent vocabulary size: Always 256 base tokens regardless of language\n",
    "\n",
    "The trade-off is that the model must learn to reconstruct valid UTF-8 from potentially fragmented byte sequences, but modern transformers handle this quite well.RetryClaude can make mistakes. Please double-check responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import regex as re\n",
    "\n",
    "def read_chunk(file_path: str, start_offset: int, end_offset: int) -> bytes:\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        f.seek(start_offset)\n",
    "        return f.read(end_offset - start_offset)\n",
    "\n",
    "def split_on_special_characaters(data: bytes, pattern: bytes) -> list[bytes]:\n",
    "    return re.split(pattern, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_tokens(token1: int | tuple[int, ...], token2: int | tuple[int, ...]) -> tuple[int, ...]:\n",
    "    if isinstance(token1, int):\n",
    "        token1 = (token1,)\n",
    "    if isinstance(token2, int):\n",
    "        token2 = (token2,)\n",
    "    return (*token1, *token2)\n",
    "\n",
    "\n",
    "def count_and_merge(pretoken_counts: collections.Counter) -> tuple[tuple[int, ...]]:\n",
    "    pair_counts = collections.Counter()\n",
    "    for word, count in pretoken_counts.items():\n",
    "        for i in range(len(word) - 1):\n",
    "            pair_counts[(word[i], word[i + 1])] += count\n",
    "\n",
    "    # Find the most common byte pair\n",
    "    # TODO: this is resolves ties arbitrarily\n",
    "    most_common_pair = pair_counts.most_common(1)[0][0]\n",
    "    pair_counts.pop(most_common_pair)\n",
    "\n",
    "    # Merge the most common pair in the pret\n",
    "    new_token = _merge_tokens(most_common_pair[0], most_common_pair[1])\n",
    "    updates = []\n",
    "    for word, count in pretoken_counts.items():\n",
    "        new_word = []\n",
    "        ix = 0\n",
    "        updated = False\n",
    "        while ix < len(word):\n",
    "            if ix + 1 < len(word) and (word[ix], word[ix + 1]) == most_common_pair:\n",
    "                new_word.append(new_token)\n",
    "                ix += 2\n",
    "                updated = True\n",
    "            else:\n",
    "                new_word.append(word[ix])\n",
    "                ix += 1\n",
    "        if updated:\n",
    "            updates.append((word, tuple(new_word), count))\n",
    "\n",
    "    # Update the pretoken_counts with the new tokens\n",
    "    for old_word, new_word, count in updates:\n",
    "        pretoken_counts.pop(old_word)\n",
    "        pretoken_counts[new_word] = count\n",
    "\n",
    "    return most_common_pair, new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patterns for the pretokenization\n",
    "PRETOKENIZATION_PATTERN = re.compile(r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "def get_chunk_boundaries(data_path: str, separator: bytes) -> list[int]:\n",
    "    with open(data_path, \"rb\") as f:\n",
    "        num_processes = 64\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "\n",
    "    return list(zip(boundaries[:-1], boundaries[1:]))\n",
    "\n",
    "\n",
    "def train_bpe(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    separator: bytes = b\"<|endoftext|>\",\n",
    "    special_tokens: list[str] = None\n",
    ") -> dict[int, tuple[bytes]]:\n",
    "    if not special_tokens:\n",
    "        special_tokens = []\n",
    "    vocabulary = collections.defaultdict(int)\n",
    "    \n",
    "    # add the special tokens to the vocabulary\n",
    "    for ix, token in enumerate(special_tokens):\n",
    "        vocabulary[token.encode(\"utf-8\")] = ix\n",
    "\n",
    "    # add initial byte tokens: assume the encoding is utf-8: 256\n",
    "    for i in range(256):\n",
    "        vocabulary[bytes([i])] = len(vocabulary)\n",
    "\n",
    "    # Find chunk boundaries\n",
    "    start_end_pairs = get_chunk_boundaries(input_path, separator=separator)\n",
    "\n",
    "    # TODO: parallelize this part\n",
    "    pretoken_counts = collections.Counter()\n",
    "\n",
    "    # Read the data in chunks\n",
    "    for start, end in start_end_pairs:\n",
    "        data = read_chunk(input_path, start, end)\n",
    "\n",
    "        # Split each chunk into stories using the EOS token\n",
    "        stories = split_on_special_characaters(data, re.escape(separator))\n",
    "\n",
    "        # Pre-tokenize the stories using a regex pattern to get individual words\n",
    "        pretokenized = re.finditer(PRETOKENIZATION_PATTERN, stories[1].decode(\"utf-8\", errors=\"ignore\"))\n",
    "\n",
    "        # Get the byte tokens for each word and merge with the global counts\n",
    "        pretoken_counts += collections.Counter(tuple(word.group(0).encode('utf-8')) for word in pretokenized if word.group(0) is not None)\n",
    "\n",
    "    # Merge the most common byte pairs until we reach the desired vocabulary size\n",
    "    merges = []\n",
    "    while len(vocabulary) < vocab_size:\n",
    "        most_common_pair, new_token = count_and_merge(pretoken_counts)\n",
    "        if not most_common_pair:\n",
    "            break\n",
    "        merges.append(most_common_pair)\n",
    "        vocabulary[new_token] = len(vocabulary)\n",
    "\n",
    "    # eos_pattern = re.escape(b\"<|endoftext|>\")\n",
    "    return vocabulary, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[(104, 101), (32, 116), (32, 97), (32, 115), (32, 119), ((32, 116), (104, 101)), (110, 100), (101, 100), (32, 98), ((32, 116), 111), ((32, 97), (110, 100)), (32, 104), (32, 84), (105, 110), (114, 101), (32, 102), (105, 116), (111, 117), ((32, 119), 97), (32, 108), (97, 121), (32, 99), (32, 100), (32, 112), (101, 114), ((32, 84), (104, 101)), (105, 115), (32, (104, 101)), (32, 109), (105, 109), ((32, 119, 97), 115), (111, 109), (111, 110), (97, 114), (97, 116), (32, 110), (105, 100), ((32, 115), 97), (32, 103), (32, 83), (105, 108), (111, 116)]\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/TinyStoriesV2-GPT4-valid.txt'\n",
    "vocab, merges = train_bpe(data_path, 300, special_tokens=['<|endoftext|>', '<|startoftext|>'])\n",
    "print(len(vocab))\n",
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
